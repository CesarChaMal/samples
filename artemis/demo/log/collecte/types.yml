tosca_definitions_version: alien_dsl_2_0_0

metadata:
  template_name: artemis.demo.log.collecte
  template_version: 2.1.0-SNAPSHOT
  template_author: alien4cloud

imports:
  - tosca-normative-types:1.0.0-ALIEN20
  - artemis.kafka.topic:2.1.0-SNAPSHOT
  - artemis.spark.pub:2.1.0-SNAPSHOT
  - artemis.hadoop.pub:2.1.0-SNAPSHOT
  - artemis.hadoop.repository:2.1.0-SNAPSHOT
  - artemis.demo.common:2.1.0-SNAPSHOT

node_types:

  artemis.demo.log.collecte.nodes.CollecteLogBatch:
    derived_from: artemis.demo.common.nodes.SparkJob
    description: |
      Module – Collecte – Batch – SPARK - Module permettant de collecter les fichiers mis à disposition par un source de donnée et les déposer dans un espace temporaire pour traitement.
    metadata:
      icon: /images/images.png
    properties:
      class_name:
        type: string
        default: demo.MockCollector
        constraints:
          - equal: demo.MockCollector
    requirements:
      - input_repository:
          capability: artemis.hadoop.pub.capabilities.HdfsRepository
          relationship: artemis.demo.common.relationships.SparkApp2HdfsRepoInput
          occurrences: [ 1, 1 ]
      - output_repository:
          capability: artemis.hadoop.pub.capabilities.HdfsRepository
          relationship: artemis.demo.common.relationships.SparkApp2HdfsRepoOutput
          occurrences: [ 1, 1 ]
      - error_repository:
          capability: artemis.hadoop.pub.capabilities.HdfsRepository
          relationship: artemis.demo.common.relationships.SparkApp2HdfsRepoError
          occurrences: [ 1, 1 ]
    attributes:
      input_file_url: { get_operation_output: [ SELF, Standard, configure, INPUT_FILE_URL ] }
      output_file_url: { get_operation_output: [ SELF, Standard, configure, OUTPUT_FILE_URL ] }
      error_file_url: { get_operation_output: [ SELF, Standard, configure, ERROR_FILE_URL ] }
      hdfs_url: { get_operation_output: [ SELF, Standard, configure, HDFS_URL ] }
      spark_url: { get_operation_output: [ SELF, Standard, configure, SPARK_URL ] }
    interfaces:
      Standard:
        configure:
          implementation: scripts/configure.sh
      tosca.interfaces.node.lifecycle.Runnable:
        run:
          inputs:
            SPARK_URL: { get_attribute: [SELF, spark_url] }
            HDFS_URL: { get_attribute: [SELF, hdfs_url] }
            CLASS_NAME: { get_property: [SELF, class_name] }
            MEMORY: { get_property: [SELF, memory] }
            CORES: { get_property: [SELF, cores] }
            INPUT_FILE_URL: { get_attribute: [SELF, input_file_url] }
            OUTPUT_FILE_URL: { get_attribute: [SELF, output_file_url] }
            ERROR_FILE_URL: { get_attribute: [SELF, error_file_url] }
            APP_ARGS: { get_property: [SELF, app_args] }
          implementation: scripts/run.sh
